---
title: 'Logistic Regression in R + Quarto Docs'
subtitle: '[Data Literacy](./index.html)'
author: 
- Matthias FrÃ¼hwirth
institute: 
- '[Institute for Retailing & Data Science](https://www.wu.ac.at/retail/)'
format:
  beamer:
    fontsize: "10pt"
    keep-tex: true
    slide-level: 2
    code-block-font-size: \tiny
    header-includes: |
      \linespread{1.15}
      \usepackage{xcolor} % load xcolor for color commands
      \setbeamertemplate{itemize item}{\raise0.5pt\hbox{\textcolor{black}{$\bullet$}}}
      \setbeamerfont{frametitle}{size=\large}
      \usepackage{etoolbox}
      \AtBeginEnvironment{verbatim}{\setlength{\parskip}{0pt}\setlength{\topsep}{0pt}}
---

## Recap and Intro

Last week

-   Causal Inference
-   DAGs & Diff-in-Diff
-   Quarto Presentations

Today

-   Logistic Regression
-   Quarto Documents

## Intro to Logistic Regression (Logit)

Logistic Regression

-   Can be used if outcome is a binary variable (e.g. 1 = Purchase Complete, 0 = Cart Abandoned)

-   Generalized Linear Model (linear regression + non-linear link function, marginal effects are non-linear)

-   Interested in modelling probabilities of outcomes:

    Q: *"How does making payment one-click affect the probability that the purchase is completed"? What about browsing time on the website?*

-   Sits right at the intersection of econometrics and machine learning

-   Widely used baseline for inference, prediction/classification with category outcomes

## Graphical

![](images/linear-regression-vs-logistic-regression.png){width="350"}

As we change X, we change the **probability** that Y becomes 1.

## Quick recap: Probability and Odds
\footnotesize

Probability $P$ is a measure that quantifies how likely an event $Y$ is to occur. It is a number between 0 (impossible) and 1 (certain).

**Example**
Consider a _fair coin toss_ where the random variable $Y$ takes on the outcomes $H$ (heads) and $T$ (tails).

- **Probability**  
  $P(Y=H) = 0.5 \qquad\qquad P(Y=T) = 0.5 = 1 - P(Y=H)$

- **Odds**  
  The odds represent the ratio of the probability that the event occurs to the probability that it does not occur. For heads:
  $$
  \text{Odds}(H) = \frac{P(Y=H)}{1 - P(Y=H)} = \frac{0.5}{0.5} = 1.
  $$

- **Log-Odds (Logit)**
  The log-odds, or logit, is the natural logarithm of the odds:
  $$
  \text{Logit}(H) = \ln\left(\frac{P(Y=H)}{1 - P(Y=H)}\right) = \ln(1) = 0.
  $$
- **Useful identity for binary outcome**
  $$
    P(Y = 1) = \frac{\text{Odds(1)}}{1 + \text{Odds(1)}}
  $$

## Basics

-   We assume that the outcome is Bernoulli distributed (outcome of a Bernoulli trial): this means with probability $p$ the outcome is 1 and with probability $1-p$ it is 0.

-   We are interested in $p$, so logistic regression models the probability of a binary outcome (Y = 0, 1). Probability Form:

$$
  P(Y = 1 \mid X) = \frac{1}{1 + e^{-\beta_0 + \beta_1 X_1 + \dots + \beta_k X_k}}
$$

-   Equivalent log odds form: The model assumes a **logit (log-odds) link** between predictors and the probability:

$$
    \log\left(\underbrace{\frac{P(Y = 1 \mid X)}{1 - P(Y = 1 \mid X)}}_{\text{Odds}}\right) = \beta_0 + \beta_1 X_1 + \dots + \beta_k X_k.
    $$


## Odds and Interpretation

\small

-   We can interpret a coefficient $\beta_k$ as changes to the log-odds; and $\exp(\beta_k)$ as changes to odds. $$
\log\left(\underbrace{\frac{P(Y = 1 \mid X)}{P(Y = 0 \mid X)}}_{\text{Odds}}\right) = \beta_0 + \beta_1 X_1 + \dots + \textcolor{blue}{\beta_k}\,\textcolor{blue}{X_k}.
$$

-   Odds reflect how likely an event is compared to it not happening ("ratio of Y=1 to Y=0"), rather than its share of all outcomes (which would be the probability).

-   **Example**: The baseline odds for 100 customers (20 purchase complete, 80 cart abandoned). Then you estimate $\textcolor{blue}{\beta_k} = 0.5$:

$$
  \text{Odds} = \frac{20}{80} = 0.25 \quad\quad \exp(\beta_k = 0.5) \approx 1.65 \quad\quad \text{New Odds} = 0.25 \times 1.65 \approx 0.4125.
$$

-   For a one-unit increase in $X_k$, the odds increase by about $(\exp(\beta) - 1) \cdot 100 \% = 65\%$. However, the effect on the actual probability depends on the starting value of the regressors.

## Probabilities

\small

-   Coefficients from the regression output can be directly interpreted as changes to (log-)odds. If $\beta_1$ \> 0, odds increase, as well as probabilities (and reverse). The _change in odds_ does not depend on the actual level of X.

-   The reason: Odds are multiplicative, log-odds are additive (changes are "just added").

-   However, the change in the outcome probability P($Y = 1 \mid X$) depends on the starting level of the covariates.

-   Remember: $$
    P(Y = 1) = \frac{\text{Odds}}{1 + \text{Odds}}
    $$

-   Previous Example (odds before: $0.25$, odds after: $0.41$). Probabilities:

$$
p_{\text{old}} = \frac{20}{20+80} = \frac{0.25}{1 + 0.25} = 0.2, \quad  p_{\text{new}} \approx \frac{0.41}{1 + 0.41} \approx 0.291.
$$

-   Interpretation: For the given baseline probability/odds, increasing the explanatory variable $X_k$ by one unit increases the probability of $Y = 1$ by approx. $9.1\%$.

## How to do in R:

\footnotesize

-   Use the `glm` (generalized linear models) function from base R.

```{r}
#| echo: true
#| eval: false
#| cache: false
logit_model <- glm(Y ~ X1 + X2, data = df, family = binomial)
summary(logit_model)
```

-   Model Fit (Pseudo-$R^2$):

```{r}
#| echo: true
#| eval: false
#| cache: false
pscl::pR2(logit_model)
```

-   Calculate predicted probabilities:

```{r}
#| echo: true
#| eval: false
#| cache: false
predict(logit_model, type = "response")
```

-   Get changes in probabilities (average marginal effects, keeps other covariates constant for each obs.)

```{r}
#| echo: true
#| eval: false
#| cache: false
#install.packages("margins")
library(margins)
marginal_effects <- margins(logit_model)
summary(marginal_effects)
```

## Example Logit: Movie Box-Office-Bombs

\small

```{r setup, include=FALSE}
options(paged.print = FALSE)
options(scipen=999)
library(tidyverse)
library(yardstick)
library(margins)
```

-   1812 movies released in U.S. cinemas between 1995 and 2024 with a budget of at least 25 million dollars.

-   We are interested in if a film is a box office flop ("bomb"). Defined as ROI \< 66% and no international success $\rightarrow$ Y = 1 (is a bomb).

-   Explanatory variables (in the data) are audience and critical scores, as well as film properties like runtime and genre.

-   Source: BoxOfficeMojo + IMDb + RottenTomatoes

::::: columns
::: column
\centering

![](images/MV5BZWNmZGYzZjUtODRmOS00ODgzLWE4NWQtMDI3MGUwNjRjYjY0XkEyXkFqcGc@._V1_FMjpg_UX1000_.jpg){width="100"}
:::

::: column
\scriptsize

**John Carter (2012)**

-   Budget: 250 million USD\
-   Domestic Box Office: 73 million USD\
-   Audience Score: 60 & Critic Score: 52 (RT), runtime: 132 minutes\
-   One of the biggest bombs of all time (Disney took a 200 million dollar write-off)
:::
:::::

## Data

\scriptsize

```{r}
#| echo: true
#| cache: false
df_movie <- read_csv("movie_select.csv") 

df_movie %>% 
  select(title,budget, domestic,runtime,audience_score,critic_score,is_bo_bomb) %>% 
  arrange(-budget) %>% head(7)
```

## Distributions

```{r}
#| echo: false
#| cache: true
#| fig-width: 4.0
#| fig-height: 4.0
#| out-width: "70%"
#| fig-align: center
library(ggplot2)
library(patchwork)
df_movie <- df_movie %>%
  mutate(is_indie = if_else(distri_type == "Independent", 1, 0))

p1 <- ggplot(df_movie, aes(x = factor(is_bo_bomb))) +
  geom_bar(fill = "steelblue") +
  labs(title = "Box Office Bomb", x = "Is Box Office Bomb", y = "Count") +
  theme_minimal()

p2 <- ggplot(df_movie, aes(x = runtime)) +
  geom_histogram(fill = "steelblue", color = "white", bins = 30) +
  labs(title = "Runtime Distribution", x = "Runtime (minutes)", y = "Count") +
  theme_minimal()

p3 <- ggplot(df_movie, aes(x = critic_score)) +
  geom_histogram(aes(y = ..density..), fill = "steelblue", color = "white", bins = 30, alpha = 0.5) +
  geom_density(color = "red", size = 1) +
  labs(title = "Critic Score Distribution", x = "Critic Score", y = "Density") +
  theme_minimal()

p4 <- ggplot(df_movie, aes(x = factor(is_indie))) +
  geom_bar(fill = "steelblue") +
  labs(title = "Indie Movies", x = "Is Indie", y = "Count") +
  theme_minimal()

combined_plot <- (p1 + p2) / (p3 + p4)
combined_plot

```

## Estimate Model

\scriptsize

```{r}
#| echo: true
#| cache: false
logit_model <- glm(is_bo_bomb ~  runtime + imdb_avg_rating + 
                     audience_score + critic_score + is_indie + 
                     ge_action + ge_animation, #+ factor(title_year),
                   data = df_movie,
                   family = binomial)

pscl::pR2(logit_model) # Print R2 (McFadden) + eval
```

## Model Summary

\tiny

```{r}
#| echo: true
#| cache: false
summary(logit_model) # Print summary
```

## Interpreting Coefficients (Odds)

\scriptsize

-   **Runtime (Estimate = 0.0103):**\
    For each additional minute of runtime, the log-odds of being a box office bomb increase by 0.0103. This corresponds to an odds ratio of $\exp(0.0103) \approx 1.01,$ meaning about a **1% increase** in the odds per minute.

-   **Audience Score (Estimate = -0.0189):**\
    Each one-point increase in the RT audience score decreases the log-odds by 0.019. This also implies a **(exp(-0.018976) - 1)\*100% = -1.879% change** (decrease) in the odds of being a box office bomb per point.

-   **Is Indie (Estimate = 0.8201):**\
    Being an indie film (when (\text{is\_indie} = 1)) increases the log-odds by 0.82 relative to non-indie films. This translates to an odds ratio of 2.27, indicating that indie films have approximately **127% higher odds** of being a box office bomb compared to non-indie films.

## Marginal Effects (on P(Y = 1))

\scriptsize

```{r}
#| echo: true
#| cache: false
marginal_effects <- margins(logit_model)
summary(marginal_effects)
```

Interpretation. On average:

-   a one-unit increase in the audience score decreases the probability that the film is a box office bomb by 0.3%.

-   a one-unit increase in the runtime increases the probability that the film is a box office bomb by 0.17%.

-   switching from a non-indie to an indie film increases the predicted probability by roughly 13.1 \[7.9, 18.2\] percentage points.

## Prediction: Predict/classify new data

\scriptsize

**Create a stinker (ong runtime and negative reviews)**

```{r}
#| echo: true
#| cache: false
new_bad_film <- data.frame(
  runtime = 240,  
  imdb_avg_rating = 6.2, audience_score = 22, critic_score = 34,    # BAD REVIEWS!
  is_indie = 1, ge_action = 0, ge_animation = 0
)

predicted_prob <- predict(logit_model, newdata = new_bad_film, type = "response")
cat("The probability of bombing is: ", predicted_prob)
```

**Create a masterpiece (great reviews and not too long)**

```{r}
#| echo: true
#| cache: false
new_great_film <- data.frame(
  runtime = 120,  
  imdb_avg_rating = 9.2, audience_score = 89, critic_score = 79,    
  is_indie = 0, ge_action = 0, ge_animation = 1
)

predicted_prob <- predict(logit_model, newdata = new_great_film, type = "response")
cat("The probability of bombing is: ", predicted_prob)
```

## Prediction and Classification (on existing data) and Model Eval

\scriptsize

-   Since logit is a common classifier, we can also evaluate our model on the quality of the models predicition (not just probabilities)

-   We can compare actual classes to predicted classes and look at how often the model is wrong ("cross entropy loss").

**Add the predicted probability as a new column and construct the predicted class (threshold 0.5)**

```{r}
#| echo: true
#| cache: false
set.seed(100)
df_movie <- df_movie %>%
  mutate(
    predicted_prob = predict(logit_model, type = "response"),
    predicted_class = if_else(predicted_prob >= 0.5, 1, 0)
  )

df_movie %>% select(title,is_bo_bomb,predicted_prob,predicted_class) %>% 
  slice_sample(n = 6)
```

## Confusion Matrix and Metrics (from package yardstick)

\scriptsize

```{r}
#| echo: true
#| cache: false
#| fig-align: center
df_movie <- df_movie %>%
  mutate(
    truth = factor(is_bo_bomb, levels = c(1, 0), labels = c("Yes", "No")),
    predicted = factor(predicted_class, levels = c(1, 0), labels = c("Yes", "No"))
  )

cm <- yardstick::conf_mat(df_movie, truth, predicted)
```

::::: columns
::: column
\vspace{1.1cm}

```{r}
#| echo: false
#| cache: false
#| fig-align: center
print(cm)
```
:::

::: column
![](confusion_matrix.png){width="144"}
:::
:::::

## Metrics

\scriptsize

There are several metrics that can be used to evaluate a binary classification model (can also use yardstick functions), which depend on these 4 values.

::::: columns
::: column
```{r}
#| echo: false
#| cache: false
#| fig-align: center
print(cm)
```
:::

::: column
```{r}
#| echo: true
TP <- 63 # True positives
TN <- 1330 # True negatives
FP <- 50 # False positives
FN <- 369 # False negatives
```
:::
:::::

\scriptsize

-   Accuracy (overall correctness):

    ```{r}
    #| echo: true
    (TP + TN) / (TP + TN + FP + FN)
    ```

-   Sensitivity/Recall: indicates how well the model identifies actual positives.

    ```{r}
    #| echo: true
    TP / (TP + FN)
    ```

-   Specificity/True Negative Rate: measures how well the model identifies actual negatives.

    ```{r}
    #| echo: true
    TN / (TN + FP)
    ```

-   Precision: reflects the accuracy of the positive predictions.

    ```{r}
    #| echo: true
    TP / (TP + FP)
    ```

## Final thoughts

-   Our model misses a lot of positives. We could experiment with lowering the threshold, if our goal is to find more positives (trade-off between recall and precision).

-   Use XGBoost or similarly flexible model (always use out-of-sample testing).

-   Probably lots of missing confounders :( 

-   We could try to add or engineer more features (regressors) like interaction terms, "text-based stuff" and crew info.


## Intro to Quarto Documents
\small
- Quarto documents are very similar to Quarto Presentations
- perfect for writing assignments, reports, your empirical thesis
- yaml (beginning of document) for PDF with Table of Contents, numbered sections and external bibliography

\footnotesize
``` yaml
---
title: "Predicting Box Office Bombs: A Logistic Regression Analysis"
author: "Matthias"
date: "`r Sys.Date()`"
format:
  pdf:
    toc: true
    number-sections: true
bibliography: references.bib
---
```
\small
- Sections and Subsections are created by using #Section and ##Subsection (similar to Quarto Pres)

## Referencing
\small

- Quarto automatically generates the âReferencesâ section at the end of your document, if you do the following:
- Create a file called `references.bib` and add it in your YAML settings (see Slide before)
- Add a reference in BibTex format to you reference file
  
  \scriptsize
  ```
  @article{smith2020,
  author = {Smith, John},
  title = {Predicting Movie Success with Machine Learning},
  journal = {Journal of Data Science},
  year = {2020},
  volume = {18},
  pages = {101-115}
  }
 ```
\small
- Cite with: 

    \footnotesize
    As shown in recent studies `[@smith2020]`, and following `@smith2020`
